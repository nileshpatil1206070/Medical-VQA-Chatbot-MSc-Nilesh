{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fully working Gradio GUI - Gemini**"
      ],
      "metadata": {
        "id": "aUN5uxl-lYOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai gradio pillow"
      ],
      "metadata": {
        "id": "ZYzotzCnlgdt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# ===========================\n",
        "# SETUP GEMINI CLIENT\n",
        "# ===========================\n",
        "API_KEY = \"AIzaSyB8gUBr7CVGHViXVIfXBOku8nGd7apimodifieddjcY4I\"\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "# ===========================\n",
        "# IMAGE RESIZE FUNCTION\n",
        "# ===========================\n",
        "def resize_image(image, max_size=1024):\n",
        "    if max(image.size) <= max_size:\n",
        "        return image\n",
        "    ratio = max_size / max(image.size)\n",
        "    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
        "    return image.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "# ===========================\n",
        "# GEMINI Q&A FUNCTION\n",
        "# ===========================\n",
        "def ask_gemini(image, question):\n",
        "    img = resize_image(image)\n",
        "\n",
        "    img_bytes = io.BytesIO()\n",
        "    img.save(img_bytes, format=\"JPEG\")\n",
        "    img_bytes = img_bytes.getvalue()\n",
        "\n",
        "    contents = [\n",
        "        types.Part.from_bytes(data=img_bytes, mime_type=\"image/jpeg\"),\n",
        "        types.Part.from_text(text=question)\n",
        "    ]\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=contents\n",
        "    )\n",
        "\n",
        "    # Limit response to 4-5 lines\n",
        "    lines = response.text.split('\\n')\n",
        "    return '\\n'.join(lines[:5])\n",
        "\n",
        "# ===========================\n",
        "# CONVERSATION MEMORY\n",
        "# ===========================\n",
        "conversation_history = []\n",
        "\n",
        "def add_to_memory(role, message):\n",
        "    conversation_history.append({\"role\": role, \"content\": message})\n",
        "\n",
        "def build_context():\n",
        "    context_text = \"\"\n",
        "    for turn in conversation_history:\n",
        "        context_text += f\"{turn['role'].capitalize()}: {turn['content']}\\n\"\n",
        "    return context_text\n",
        "\n",
        "# ===========================\n",
        "# SUMMARIZATION FUNCTION\n",
        "# ===========================\n",
        "def summarize_conversation():\n",
        "    context_text = build_context()\n",
        "    prompt = f\"Summarize the following medical conversation in concise clinical language:\\n{context_text}\"\n",
        "\n",
        "    contents = [types.Part.from_text(text=prompt)]\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=contents\n",
        "    )\n",
        "    lines = response.text.split('\\n')\n",
        "    return '\\n'.join(lines[:5])  # limit to 4-5 lines\n",
        "\n",
        "# ===========================\n",
        "# DIAGNOSTIC / TREATMENT FUNCTION\n",
        "# ===========================\n",
        "def suggest_diagnosis_treatment():\n",
        "    context_text = build_context()\n",
        "    prompt = f\"Based on the following medical conversation, provide possible diagnostics and treatment suggestions in paragraph form:\\n{context_text}\"\n",
        "\n",
        "    contents = [types.Part.from_text(text=prompt)]\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=contents\n",
        "    )\n",
        "    return response.text  # full paragraph\n",
        "\n",
        "# ===========================\n",
        "# MULTI-TURN CHAT FUNCTION\n",
        "# ===========================\n",
        "def medical_chat(image, user_message):\n",
        "    add_to_memory(\"user\", user_message)\n",
        "    gemini_response = ask_gemini(image, user_message)\n",
        "    add_to_memory(\"assistant\", gemini_response)\n",
        "    return gemini_response\n",
        "\n"
      ],
      "metadata": {
        "id": "4_QqaOkklDzU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# GRADIO INTERFACE\n",
        "# ===========================\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Multi-turn Medical Visual Chatbot-VLM-gemini ðŸ¤–ðŸ©º\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Medical Scan\")\n",
        "            user_input = gr.Textbox(lines=2, placeholder=\"Ask your question here...\", label=\"Question\")\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "        with gr.Column():\n",
        "            chat_output = gr.Textbox(label=\"Assistant Answer\", interactive=False, lines=10)\n",
        "            summarize_btn = gr.Button(\"Summarize Conversation\")\n",
        "            summary_output = gr.Textbox(label=\"Conversation Summary\", interactive=False, lines=10)\n",
        "            diagnosis_btn = gr.Button(\"Suggest Diagnosis/Treatment\")\n",
        "            diagnosis_output = gr.Textbox(label=\"Diagnostics & Treatment\", interactive=False, lines=15)\n",
        "\n",
        "    send_btn.click(fn=medical_chat, inputs=[image_input, user_input], outputs=chat_output)\n",
        "    summarize_btn.click(fn=summarize_conversation, inputs=[], outputs=summary_output)\n",
        "    diagnosis_btn.click(fn=suggest_diagnosis_treatment, inputs=[], outputs=diagnosis_output)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "Yc5e1R2RlD1o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "a60c3602-299a-4572-f58f-2571f5f4302e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a64a8c41249b21bdcb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a64a8c41249b21bdcb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCTsfMO4lD30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mUlGc9RlD6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yDVc23GwlD9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5aXgNXs5lNfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qvxM27ClNhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6ls_6AhlNkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2eHDzCrlNmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wP84vO_lNpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66nnieHBlNrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1e3Yl1rlNtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7snu-dYLlNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "efFzzCPjlNyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
