# -*- coding: utf-8 -*-
"""Progress_till_far_Med_VQA_model_training_GUI_testing_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_oENn5rMaMLk9v1-VoktnyjKaRcHzx74

**MODEL Training on small dataset and GUI testing**
"""

!pip install transformers datasets accelerate bitsandbytes gradio pillow

from datasets import load_dataset
from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments
from PIL import Image
import torch
import gradio as gr

dataset = load_dataset("jspetrisko/slake-simplified", split="train[:1000]")

# Preview
print(dataset.column_names)
print(dataset[0])

model_name = "Salesforce/blip-vqa-base"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)


def preprocess(example):
    image = example["image"].convert("RGB")
    question = example["query"]
    answer = example["answers"]

    inputs = processor(images=image, text=question, padding="max_length", truncation=True, return_tensors="pt")
    inputs["labels"] = processor.tokenizer(answer, padding="max_length", truncation=True, return_tensors="pt").input_ids
    return {k: v.squeeze() for k, v in inputs.items()}

processed_dataset = dataset.map(preprocess)

from transformers import AutoProcessor
processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

def preprocess(batch):
    inputs = processor(images=batch["image"], text=batch["query"], return_tensors="pt", padding=True)
    return inputs

dataset = dataset.map(preprocess, batched=True)

from torchvision import transforms
from PIL import Image

# Define transform to convert images to tensors and resize
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # match model input size
    transforms.ToTensor()
])

def preprocess(batch):
    images = []
    for img in batch["image"]:
        if isinstance(img, Image.Image):
            img = transform(img)
        images.append(img)
    batch["pixel_values"] = images
    return batch

dataset = dataset.map(preprocess, batched=True)



"""**Model Training and parameters adjusting**"""

# =====================================
# STEP 6: Training Arguments
# =====================================
import os
os.environ["WANDB_DISABLED"] = "true"


training_args = TrainingArguments(
    output_dir="./blip-medvqa",
    per_device_train_batch_size=2,
    num_train_epochs=1,
    learning_rate=5e-5,
    fp16=True,
    save_total_limit=1,
    logging_steps=20,
    remove_unused_columns=False,
)
# =====================================
# STEP 7: Trainer Setup
# =====================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset,
)

# =====================================
# STEP 8: Train the Model
# =====================================
trainer.train()

# =====================================
# STEP 9: Save Model & Processor
# =====================================
model.save_pretrained("./blip-medvqa_mymodel")
processor.save_pretrained("./blip-medvqa_myodel")

"""**Gradio GUI Testing**"""

# =====================================
# STEP 10: Inference Function
# =====================================
def answer_question(image, question):
    image = image.convert("RGB")
    inputs = processor(images=image, text=question, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    model.to(inputs["input_ids"].device)
    out = model.generate(**inputs, max_length=50)
    answer = processor.decode(out[0], skip_special_tokens=True)
    return answer

# =====================================
# STEP 11: Gradio Interface
# =====================================
interface = gr.Interface(
    fn=answer_question,
    inputs=[gr.Image(type="pil"), gr.Textbox(label="Ask a medical question")],
    outputs=gr.Textbox(label="Model's Answer"),
    title="ðŸ§  Medical Visual Question Answering (BLIP-MedVQA)",
    description="Upload a radiology image (e.g., X-ray, MRI) and ask a question. The model will provide an answer."
)

interface.launch(share=True)

"""***New code after fixing errors***"""

!pip install transformers datasets accelerate bitsandbytes gradio pillow

# =====================================
# STEP 1: Imports
# =====================================
from datasets import load_dataset
from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments
from PIL import Image
from torchvision import transforms
import torch
import gradio as gr
import gc

# =====================================
# STEP 2: Disable wandb logging (optional)
# =====================================
import os
os.environ["WANDB_DISABLED"] = "true"

# =====================================
# STEP 3: Load SLAKE Dataset (limit to 1000 rows)
# =====================================
dataset = load_dataset("jspetrisko/slake-simplified", split="train[:1000]")

print("Columns:", dataset.column_names)
print("Sample:", dataset[0])

# =====================================
# STEP 4: Define Image Preprocessing
# =====================================
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

def preprocess(example):
    img = example["image"]
    if isinstance(img, Image.Image):
        example["pixel_values"] = transform(img)
    example["text"] = example["query"]
    example["labels_text"] = example["answers"]
    return example

processed_dataset = dataset.map(preprocess, batched=False)  # batched=False saves memory

# Delete original dataset to free RAM
del dataset
gc.collect()

# =====================================
# STEP 5: Load Pretrained BLIP Model
# =====================================
model_name = "Salesforce/blip-vqa-base"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

# =====================================
# STEP 6: Prepare TrainingArguments
# =====================================
training_args = TrainingArguments(
    output_dir="./blip-medvqa",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=5e-5,
    fp16=True,
    logging_steps=10,
    save_total_limit=1,
    remove_unused_columns=False  # <-- crucial
)

from transformers import BlipForConditionalGeneration

class BlipWrapper(BlipForConditionalGeneration):
    def forward(self, *args, **kwargs):
        # Remove any unsupported keys
        kwargs.pop("num_items_in_batch", None)
        return super().forward(*args, **kwargs)

model = BlipWrapper.from_pretrained("Salesforce/blip-vqa-base").to("cuda" if torch.cuda.is_available() else "cpu")

# =====================================
# STEP 7: Prepare Dataset for Trainer
# =====================================
from torch.utils.data import Dataset

class MedVQADataset(Dataset):
    def __init__(self, dataset, processor):
        self.dataset = dataset
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        encoding = self.processor(images=item["image"],
                                  text=item["text"],
                                  return_tensors="pt",
                                  padding="max_length",
                                  truncation=True)
        labels = self.processor.tokenizer(item["labels_text"],
                                          padding="max_length",
                                          truncation=True,
                                          return_tensors="pt").input_ids
        encoding["labels"] = labels.squeeze()
        encoding = {k: v.squeeze() for k, v in encoding.items()}
        return encoding

train_dataset = MedVQADataset(processed_dataset, processor)

# =====================================
# STEP 8: Trainer Setup
# =====================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

# =====================================
# STEP 9: Train Model
# =====================================
trainer.train()

# =====================================
# STEP 10: Save Model & Processor
# =====================================
model.save_pretrained("./blip-medvqa_mymodel")
processor.save_pretrained("./blip-medvqa_mymodel")

# =====================================
# STEP 11: Inference Function
# =====================================
def answer_question(image, question):
    image = image.convert("RGB")
    inputs = processor(images=image, text=question, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    model.to(inputs["input_ids"].device)
    out = model.generate(**inputs, max_length=50)
    answer = processor.decode(out[0], skip_special_tokens=True)
    return answer

# =====================================
# STEP 12: Gradio Interface
# =====================================
interface = gr.Interface(
    fn=answer_question,
    inputs=[gr.Image(type="pil"), gr.Textbox(label="Ask a medical question")],
    outputs=gr.Textbox(label="Model Answer"),
    title="ðŸ§  Medical Visual Question Answering (BLIP-MedVQA)",
    description="Upload a radiology image (e.g., X-ray, MRI) and ask a question. The model will provide an answer."
)

interface.launch(share=True)



"""**Gradio GUI for pretrained model- Demo**"""

pip install git+https://github.com/huggingface/transformers.git

from transformers import BlipProcessor, BlipForQuestionAnswering
import gradio as gr
from PIL import Image
import torch

# âœ… Publicly available model
model_name = "Salesforce/blip-vqa-base"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForQuestionAnswering.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

def med_vqa(image, question):
    # Add a medical context prompt for better relevance
    question = f"This is a medical image. {question}"
    inputs = processor(image, question, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=50)
    answer = processor.decode(outputs[0], skip_special_tokens=True)
    return answer

# Gradio GUI
demo = gr.Interface(
    fn=med_vqa,
    inputs=[
        gr.Image(type="pil", label="Upload Medical Image"),
        gr.Textbox(label="Ask a Medical Question")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="ðŸ©º Medical Visual QA Demo (BLIP Base)",
    description="Upload an X-ray or CT image and ask a medical question. Model: Salesforce/blip-vqa-base"
)

demo.launch(share=True)

"""# **Testing new VLM - more general - descriptive-20/11/25**"""

!pip install transformers torch torchvision pillow gradio --quiet

from transformers import BlipProcessor, BlipForConditionalGeneration
import gradio as gr
from PIL import Image
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load BLIP image captioning model
caption_model_name = "Salesforce/blip-image-captioning-base"
processor = BlipProcessor.from_pretrained(caption_model_name)
model = BlipForConditionalGeneration.from_pretrained(caption_model_name).to(device)

def describe_image(image):
    """
    Given a medical image (e.g., X-ray), produce a descriptive caption.
    """
    # You can add a hint in the text to make it more medical:
    prompt = "A medical scan shows: "
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)
    out_ids = model.generate(**inputs, max_new_tokens=50)
    description = processor.decode(out_ids[0], skip_special_tokens=True)
    return description

# Gradio app for demonstration
demo = gr.Interface(
    fn=describe_image,
    inputs=[gr.Image(type="pil", label="Upload Medical Image")],
    outputs=gr.Textbox(label="Description of Image"),
    title="Medical Image Description Demo",
    description="Upload a radiology image (X-ray, CT, etc.) â€” the model will describe what it sees."
)

demo.launch(share=True)



import gradio as gr
from transformers import AutoProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# ---------------------------
# Load Model
# ---------------------------
model_name = "WafaaFraih/blip-roco-radiology-captioning"

device = "cuda" if torch.cuda.is_available() else "cpu"

processor = AutoProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)


# ---------------------------
# Caption Function
# ---------------------------
def describe_medical_image(image):
    if image is None:
        return "Please upload an image."

    inputs = processor(images=image, return_tensors="pt").to(device)

    output_ids = model.generate(
        **inputs,
        max_new_tokens=70,
        num_beams=5,
        repetition_penalty=1.2
    )

    caption = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

    return caption

# ---------------------------
# Gradio UI
# ---------------------------
demo = gr.Interface(
    fn=describe_medical_image,
    inputs=gr.Image(type="pil", label="Upload Medical Image (X-ray, CT, MRI, etc.)"),
    outputs=gr.Textbox(label="AI-Generated Medical Description"),
    title="ðŸ©º Medical Image Captioning (Radiology)",
    description=(
        "Upload any medical image and the model will generate a radiology-style "
        "caption trained on the ROCO dataset. Model: WafaaFraih/blip-roco-radiology-captioning"
    ),
    examples=[
        ["example_xray.png"],
        ["example_ct.png"]
    ],
)

demo.launch(share=True)







"""# **Medical caption - QA + Explanation**"""

import gradio as gr
from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
from PIL import Image
import torch

# ---------------------------
# Load Captioning Model (BLIP ROCO Radiology)
# ---------------------------
caption_model_name = "WafaaFraih/blip-roco-radiology-captioning"
device = "cuda" if torch.cuda.is_available() else "cpu"

caption_processor = AutoProcessor.from_pretrained(caption_model_name)
caption_model = BlipForConditionalGeneration.from_pretrained(caption_model_name).to(device)

# ---------------------------
# Load Text Expansion LLM (Small, Open-Source)
# ---------------------------
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

expand_model_name = "tiiuae/Falcon-H1-0.5B-Instruct"

expand_tokenizer = AutoTokenizer.from_pretrained(expand_model_name)
expand_model = AutoModelForCausalLM.from_pretrained(
    expand_model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

def expand_caption_to_paragraph(caption: str) -> str:
    prompt = (
        "You are a medical expert. Expand this radiology caption into a clinically relevant paragraph:\n"
        f"Caption: {caption}\n\nExplanation:"
    )
    inputs = expand_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to(expand_model.device)
    outputs = expand_model.generate(
        **inputs,
        max_new_tokens=180,
        temperature=0.7,
        top_p=0.95,
        do_sample=True
    )
    return expand_tokenizer.decode(outputs[0], skip_special_tokens=True)






# ---------------------------
# Generate Caption
# ---------------------------
def generate_caption(image):
    inputs = caption_processor(images=image, return_tensors="pt").to(device)
    output_ids = caption_model.generate(
        **inputs,
        max_new_tokens=70,
        num_beams=5,
        repetition_penalty=1.2,
    )
    caption = caption_processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    return caption

# ---------------------------
# Expand Caption into a 4â€“6 Line Medical Paragraph
# ---------------------------
def expand_caption(caption):
    prompt = (
        "Rewrite the following medical image caption into a detailed 4-6 sentence "
        "clinical paragraph with medically relevant explanations:\n\n"
        f"Caption: {caption}\n\nExpanded Explanation:"
    )

    inputs = expand_tokenizer(prompt, return_tensors="pt").to(device)
    outputs = expand_model.generate(
        **inputs,
        max_new_tokens=200,
        num_beams=5,
        temperature=0.7,
    )
    paragraph = expand_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return paragraph

# ---------------------------
# Pipeline: Image â†’ Caption â†’ Expanded Paragraph
# ---------------------------
def full_pipeline(image):
    if image is None:
        return "Please upload an image.", ""

    caption = generate_caption(image)
    expanded = expand_caption(caption)

    return caption, expanded

# ---------------------------
# Gradio Interface
# ---------------------------

import gradio as gr
from transformers import AutoProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM
from PIL import Image
import torch
demo = gr.Interface(
    fn=full_pipeline,
    inputs=gr.Image(type="pil", label="Upload Medical Image (X-ray, CT, MRI)"),
    outputs=[
        gr.Textbox(label="Model Caption (BLIP-Radiology)"),
        gr.Textbox(label="Expanded 4â€“6 Line Medical Explanation")
    ],
    title="ðŸ©º Medical Image Explanation AI",
    description=(
        "This tool uses BLIP Radiology Captioning to detect medical findings and "
        "new model to expand it into a clinical paragraph explanation."
    ),
)

demo.launch(debug=True)

# ---------------------------
# Gradio Interface
# ---------------------------
demo = gr.Interface(
    fn=full_pipeline,
    inputs=gr.Image(type="pil", label="Upload Medical Image (X-ray, CT, MRI)"),
    outputs=[
        gr.Textbox(label="Model Caption (BLIP-Radiology)"),
        gr.Textbox(label="Expanded 4â€“6 Line Medical Explanation")
    ],
    title="ðŸ©º Medical Image Explanation AI",
    description=(
        "This tool uses BLIP Radiology Captioning to detect medical findings and "
        "FLAN-T5 to expand it into a clinical paragraph explanation."
    ),
)

demo.launch(share=True)















