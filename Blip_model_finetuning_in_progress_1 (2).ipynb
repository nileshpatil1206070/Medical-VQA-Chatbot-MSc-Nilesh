{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPbUIqDpqH4B"
      },
      "source": [
        "**MODEL Training on small dataset and GUI testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq5MISuSv9Jd"
      },
      "source": [
        "***New code after fixing errors - SLAKE DATASET***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kqsIn0tiqHNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d9b5a1-175d-4a9a-eeee-540345ce1469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lets load all required libraries\n"
          ]
        }
      ],
      "source": [
        "print(\"lets load all required libraries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74cyNUNRwCjL"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# STEP 1: Imports\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import gradio as gr\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtDQRWK2wCln",
        "outputId": "0076b57f-8680-43c7-b819-b082e69cf413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns: ['image', 'query', 'answers']\n",
            "Sample: {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x7FF1ED4870E0>, 'query': 'What modality is used to take this image?', 'answers': 'MRI'}\n"
          ]
        }
      ],
      "source": [
        "# =====================================\n",
        "# STEP 2: Disable wandb logging\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# =====================================\n",
        "# STEP 3: Load SLAKE Dataset from huggingspace\n",
        "\n",
        "dataset = load_dataset(\"jspetrisko/slake-simplified\", split=\"train[:1000]\")\n",
        "\n",
        "print(\"Columns:\", dataset.column_names)\n",
        "print(\"Sample:\", dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "90417709ed3c48f1ac71170371f5c139",
            "49252686c20d42b8a186ccfc186b7f45",
            "000c983b53504ba99963fc501e72bc19",
            "77991737bc39424b9a4effe94f086a69",
            "168ad727f6084a19aa20b47ea1da2cec",
            "5e1f22b650c24935983035eba3a3bdb0",
            "39b7ef0208a44786a6d24eaec4948f8f",
            "3473cfade06441c38eaf3fa65d5d92cc",
            "bbaed94f805647a9bca795ee3432d250",
            "7425dd0c01c142b9ab8206549377e097",
            "18073854c6bd4d72bb91b159a67717fe"
          ]
        },
        "id": "Lza5A3WYwCoM",
        "outputId": "c6edf822-1c10-40c7-861a-88d140cc551a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90417709ed3c48f1ac71170371f5c139",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "335"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =====================================\n",
        "# STEP 4: Define Image Preprocessing\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def preprocess(example):\n",
        "    img = example[\"image\"]\n",
        "    if isinstance(img, Image.Image):\n",
        "        example[\"pixel_values\"] = transform(img)\n",
        "    example[\"text\"] = example[\"query\"]\n",
        "    example[\"labels_text\"] = example[\"answers\"]\n",
        "    return example\n",
        "\n",
        "processed_dataset = dataset.map(preprocess, batched=False)  # batched=False saves memory\n",
        "\n",
        "# Delete original dataset to free RAM\n",
        "del dataset\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ActQ3y5DwizI",
        "outputId": "fdea97ae-9025-4695-b435-0bea989617c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "# =====================================\n",
        "# STEP 5: Load Pretrained BLIP Model\n",
        "# =====================================\n",
        "model_name = \"Salesforce/blip-vqa-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "N7vz_UnDwCql",
        "outputId": "ffb373d8-56fe-42e5-dc1b-8b901397d1f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='282' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 282/1000 1:29:16 < 3:48:55, 0.05 it/s, Epoch 0.28/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>9.705500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>8.087300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>6.901000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.593100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.042100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.045300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.029300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.032400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.021300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.030500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.192900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.014600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.011900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.010800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.017600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# =====================================\n",
        "# STEP 6: Prepare TrainingArguments\n",
        "# =====================================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./blip-medvqa\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    remove_unused_columns=False  # <-- crucial\n",
        ")\n",
        "\n",
        "from transformers import BlipForConditionalGeneration\n",
        "\n",
        "class BlipWrapper(BlipForConditionalGeneration):\n",
        "    def forward(self, *args, **kwargs):\n",
        "        # Remove any unsupported keys\n",
        "        kwargs.pop(\"num_items_in_batch\", None)\n",
        "        return super().forward(*args, **kwargs)\n",
        "\n",
        "model = BlipWrapper.from_pretrained(\"Salesforce/blip-vqa-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =====================================\n",
        "# STEP 7: Prepare Dataset for Trainer\n",
        "# =====================================\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MedVQADataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(images=item[\"image\"],\n",
        "                                  text=item[\"text\"],\n",
        "                                  return_tensors=\"pt\",\n",
        "                                  padding=\"max_length\",\n",
        "                                  truncation=True)\n",
        "        labels = self.processor.tokenizer(item[\"labels_text\"],\n",
        "                                          padding=\"max_length\",\n",
        "                                          truncation=True,\n",
        "                                          return_tensors=\"pt\").input_ids\n",
        "        encoding[\"labels\"] = labels.squeeze()\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        return encoding\n",
        "\n",
        "train_dataset = MedVQADataset(processed_dataset, processor)\n",
        "\n",
        "# =====================================\n",
        "# STEP 8: Trainer Setup\n",
        "# =====================================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# STEP 9: Train Model\n",
        "# =====================================\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzVuZIWkwCs_"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# STEP 10: Save Model & Processor\n",
        "# =====================================\n",
        "model.save_pretrained(\"./blip-medvqa_mymodel\")\n",
        "processor.save_pretrained(\"./blip-medvqa_mymodel\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc1QPNuDwCvr"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# STEP 11: Inference Function\n",
        "# =====================================\n",
        "def answer_question(image, question):\n",
        "    image = image.convert(\"RGB\")\n",
        "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(inputs[\"input_ids\"].device)\n",
        "    out = model.generate(**inputs, max_length=50)\n",
        "    answer = processor.decode(out[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# =====================================\n",
        "# STEP 12: Gradio Interface\n",
        "# =====================================\n",
        "interface = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=[gr.Image(type=\"pil\"), gr.Textbox(label=\"Ask a medical question\")],\n",
        "    outputs=gr.Textbox(label=\"Model Answer\"),\n",
        "    title=\"ðŸ§  Medical Visual Question Answering (BLIP-MedVQA)\",\n",
        "    description=\"Upload a radiology image (e.g., X-ray, MRI) and ask a question. The model will provide an answer.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AmTChQjN7d9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJXwmJjJ7eAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJiAQJ4S7eBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhBYk3Ye7eFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4I2ieSM7eGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDAmstpP7eKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrmlaaKJ7eLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCceeqYwC3Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
        