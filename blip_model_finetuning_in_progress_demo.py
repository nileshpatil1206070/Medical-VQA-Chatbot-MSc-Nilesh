# -*- coding: utf-8 -*-
"""Blip_model_finetuning_in_progress_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YXN454FigupcKm7jWv2nU0zctV49ofTN

**MODEL Training on small dataset and GUI testing**

***New code after fixing errors - SLAKE DATASET***
"""



# =====================================
# STEP 1: Imports

from datasets import load_dataset
from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments
from PIL import Image
from torchvision import transforms
import torch
import gradio as gr
import gc

# =====================================
# STEP 2: Disable wandb logging
import os
os.environ["WANDB_DISABLED"] = "true"

# =====================================
# STEP 3: Load SLAKE Dataset from huggingspace

dataset = load_dataset("jspetrisko/slake-simplified", split="train[:1000]")

print("Columns:", dataset.column_names)
print("Sample:", dataset[0])

# =====================================
# STEP 4: Define Image Preprocessing

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

def preprocess(example):
    img = example["image"]
    if isinstance(img, Image.Image):
        example["pixel_values"] = transform(img)
    example["text"] = example["query"]
    example["labels_text"] = example["answers"]
    return example

processed_dataset = dataset.map(preprocess, batched=False)  # batched=False saves memory

# Delete original dataset to free RAM
del dataset
gc.collect()

# =====================================
# STEP 5: Load Pretrained BLIP Model
# =====================================
model_name = "Salesforce/blip-vqa-base"
processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

# =====================================
# STEP 6: Prepare TrainingArguments
# =====================================
training_args = TrainingArguments(
    output_dir="./blip-medvqa",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    learning_rate=5e-5,
    fp16=True,
    logging_steps=10,
    save_total_limit=1,
    remove_unused_columns=False  # <-- crucial
)

from transformers import BlipForConditionalGeneration

class BlipWrapper(BlipForConditionalGeneration):
    def forward(self, *args, **kwargs):
        # Remove any unsupported keys
        kwargs.pop("num_items_in_batch", None)
        return super().forward(*args, **kwargs)

model = BlipWrapper.from_pretrained("Salesforce/blip-vqa-base").to("cuda" if torch.cuda.is_available() else "cpu")

# =====================================
# STEP 7: Prepare Dataset for Trainer
# =====================================
from torch.utils.data import Dataset

class MedVQADataset(Dataset):
    def __init__(self, dataset, processor):
        self.dataset = dataset
        self.processor = processor

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        item = self.dataset[idx]
        encoding = self.processor(images=item["image"],
                                  text=item["text"],
                                  return_tensors="pt",
                                  padding="max_length",
                                  truncation=True)
        labels = self.processor.tokenizer(item["labels_text"],
                                          padding="max_length",
                                          truncation=True,
                                          return_tensors="pt").input_ids
        encoding["labels"] = labels.squeeze()
        encoding = {k: v.squeeze() for k, v in encoding.items()}
        return encoding

train_dataset = MedVQADataset(processed_dataset, processor)

# =====================================
# STEP 8: Trainer Setup
# =====================================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
)

# =====================================
# STEP 9: Train Model
# =====================================
trainer.train()

# =====================================
# STEP 10: Save Model & Processor
# =====================================
model.save_pretrained("./blip-medvqa_mymodel")
processor.save_pretrained("./blip-medvqa_mymodel")

# =====================================
# STEP 11: Inference Function
# =====================================
def answer_question(image, question):
    image = image.convert("RGB")
    inputs = processor(images=image, text=question, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    model.to(inputs["input_ids"].device)
    out = model.generate(**inputs, max_length=50)
    answer = processor.decode(out[0], skip_special_tokens=True)
    return answer

# =====================================
# STEP 12: Gradio Interface
# =====================================
interface = gr.Interface(
    fn=answer_question,
    inputs=[gr.Image(type="pil"), gr.Textbox(label="Ask a medical question")],
    outputs=gr.Textbox(label="Model Answer"),
    title="ðŸ§  Medical Visual Question Answering (BLIP-MedVQA)",
    description="Upload a radiology image (e.g., X-ray, MRI) and ask a question. The model will provide an answer."
)

interface.launch(share=True)















