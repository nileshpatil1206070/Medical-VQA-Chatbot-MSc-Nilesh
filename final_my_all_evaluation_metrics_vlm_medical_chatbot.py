# -*- coding: utf-8 -*-
"""Final_my_All_evaluation_metrics_VLM_medical_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1as86JEUWBzb-cCVtbrPv29dTOwefS-I9
"""



!pip install nltk rouge-score bert-score scikit-learn pillow gradio

import gradio as gr
import numpy as np
from PIL import Image
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from scipy.stats import ttest_rel
import gradio as gr
import numpy as np
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from bert_score import score as bert_score
from scipy.stats import ttest_rel
from PIL import Image

# -------------------------------
# Metrics computation
# -------------------------------
rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)

def compute_metrics(preds, golds):
    acc = np.mean([p.strip().lower() == g.strip().lower() for p, g in zip(preds, golds)])
    bleu = np.mean([sentence_bleu([g.split()], p.split()) for p, g in zip(preds, golds)])
    rouge1 = np.mean([rouge.score(g, p)["rouge1"].fmeasure for p, g in zip(preds, golds)])
    rougel = np.mean([rouge.score(g, p)["rougeL"].fmeasure for p, g in zip(preds, golds)])
    P, R, F = bert_score(preds, golds, lang="en", verbose=False)
    return {
        "accuracy": acc,
        "bleu": bleu,
        "rouge1": rouge1,
        "rougel": rougel,
        "bert_f1": float(F.mean())
    }

# -------------------------------
# Evaluation function
# -------------------------------
def evaluate_answers(chatgpt_answers, gemini_answers, finetuned_answers, gold_answers):
    chatgpt_list = [x.strip() for x in chatgpt_answers.strip().split("\n") if x.strip()]
    gemini_list = [x.strip() for x in gemini_answers.strip().split("\n") if x.strip()]
    finetuned_list = [x.strip() for x in finetuned_answers.strip().split("\n") if x.strip()]
    gold_list = [x.strip() for x in gold_answers.strip().split("\n") if x.strip()]

    n = len(gold_list)
    if not (len(chatgpt_list) == len(gemini_list) == len(finetuned_list) == n):
        return "Error: Number of answers and golds must match for all models."

    results = {
        "ChatGPT": compute_metrics(chatgpt_list, gold_list),
        "Gemini": compute_metrics(gemini_list, gold_list),
        "Finetuned": compute_metrics(finetuned_list, gold_list)
    }

    return results

# -------------------------------
# Gradio Interface
# -------------------------------
with gr.Blocks() as demo:
    gr.Markdown("## Multi-Model Answer Evaluation ðŸ§ª")

    with gr.Row():
        with gr.Column():
            image_input = gr.Image(type="pil", label="Upload Medical Image")
            question_input = gr.Textbox(lines=2, placeholder="Type your question here", label="Question")
            chatgpt_input = gr.Textbox(lines=5, placeholder="Paste ChatGPT answers (one per line)", label="ChatGPT Answers")
            gemini_input = gr.Textbox(lines=5, placeholder="Paste Gemini answers (one per line)", label="Gemini Answers")
            finetuned_input = gr.Textbox(lines=5, placeholder="Paste Finetuned answers (one per line)", label="Finetuned Answers")
            gold_input = gr.Textbox(lines=5, placeholder="Paste Gold answers (one per line)", label="Gold Answers")
            eval_btn = gr.Button("Compute Metrics")

        with gr.Column():
            metrics_output = gr.JSON(label="Metrics per Model")

    eval_btn.click(
        evaluate_answers,
        inputs=[chatgpt_input, gemini_input, finetuned_input, gold_input],
        outputs=metrics_output
    )

demo.launch()







